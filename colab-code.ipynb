{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saarthi.ai Case Study.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEicEaVp1XSY",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cijGMCZgzhxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "PATH = r'/content/drive/My Drive/Saarthi.ai Assignment/'\n",
        "os.chdir(PATH)\n",
        "import librosa\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules import Module\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "# !pip install soundfile\n",
        "\n",
        "import soundfile\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjeMxsdQ1cVS",
        "colab_type": "text"
      },
      "source": [
        "# Audio Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bMKJM5o-aWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Audio inputted is sliced into segments of length 8192, with 50% overlap for train set, 0% overlap for inference. Padded 0s for packing the last 8192 samples.\n",
        "\n",
        "def slice_signal(filepath, window_size, stride, sample_rate = 8000):\n",
        "  wav, sr = librosa.load(filepath, sr = sample_rate)\n",
        "  if sr != 8000: print(f'Sampling rate error. Current rate is {sr}')\n",
        "  n_samples = wav.shape[0]  # contains simple amplitudes\n",
        "  hop = int(window_size * stride)\n",
        "  slices = []\n",
        "  wav = np.append(wav, [0] * (8192 - (len(wav) % 8192)))\n",
        "  for end_idx in range(window_size, len(wav) + 1, hop):\n",
        "      start_idx = end_idx - window_size\n",
        "      slice_sig = wav[start_idx:end_idx]\n",
        "      slices.append(slice_sig)\n",
        "  return slices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWdpaFWlCA1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split audio each file's segments into single batches (.npy files) for easy loading during training.\n",
        "\n",
        "def serialize():\n",
        "\n",
        "  start_time = time.time()\n",
        "  window_size = 2 ** 13 # 8192 samples\n",
        "  sample_rate = 8000\n",
        "  stride = 0.5\n",
        "\n",
        "  clean_data_path = f'{PATH}/clean_trainset_28spk_wav/'\n",
        "  noisy_data_path = f'{PATH}/noisy_trainset_28spk_wav/'\n",
        "\n",
        "  for file in os.listdir(clean_data_path):\n",
        "\n",
        "    clean_filepath = f'{clean_data_path}{file}'\n",
        "    noisy_filepath = f'{noisy_data_path}{file}'\n",
        "\n",
        "    clean_sliced = slice_signal(clean_filepath, window_size, stride, sample_rate)\n",
        "    noisy_sliced = slice_signal(noisy_filepath, window_size, stride, sample_rate)\n",
        "\n",
        "    for idx, slice_tuple in enumerate(zip(clean_sliced, noisy_sliced)):\n",
        "      pair = np.array([slice_tuple[0], slice_tuple[1]])\n",
        "      np.save(os.path.join(f'{PATH}/serialized', '{}_{}'.format(file, idx)), arr=pair)\n",
        "\n",
        "  end_time = time.time()\n",
        "  print('Total elapsed time for preprocessing : {}'.format(end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCuavw_8Ey01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74cf606-e1d7-4ffe-9830-0981fe22a170"
      },
      "source": [
        "serialize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total elapsed time for preprocessing : 14714.036752700806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8guBf_7Mbt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-emphasis filter\n",
        "\n",
        "pre_emphasis = lambda batch: signal.lfilter([1, -0.95], [1], batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV3t6-_UM55Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "de_emphasis = lambda batch: signal.lfilter([1], [1, -0.95], batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRy8x5Q01hzZ",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHtsNWlcHCKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloader from https://github.com/dansuh17/segan-pytorch\n",
        "\n",
        "class AudioSampleGenerator(data.Dataset):\n",
        "  \"\"\"\n",
        "  Audio sample reader.\n",
        "  Used alongside with DataLoader class to generate batches.\n",
        "  see: http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\n",
        "  \"\"\"\n",
        "  SAMPLE_LENGTH = 8192\n",
        "\n",
        "  def __init__(self, data_folder_path: str):\n",
        "    if not os.path.exists(data_folder_path):\n",
        "      raise FileNotFoundError\n",
        "\n",
        "    # store full paths - not the actual files.\n",
        "    # all files cannot be loaded up to memory due to its large size.\n",
        "    # insted, we read from files upon fetching batches (see __getitem__() implementation)\n",
        "    self.filepaths = [os.path.join(data_folder_path, filename) for filename in os.listdir(data_folder_path)]\n",
        "    self.num_data = len(self.filepaths)\n",
        "\n",
        "  def reference_batch(self, batch_size: int):\n",
        "    \"\"\"\n",
        "    Randomly selects a reference batch from dataset.\n",
        "    Reference batch is used for calculating statistics for virtual batch normalization operation.\n",
        "    Args:\n",
        "        batch_size(int): batch size\n",
        "    Returns:\n",
        "        ref_batch: reference batch\n",
        "    \"\"\"\n",
        "    ref_filenames = np.random.choice(self.filepaths, batch_size)\n",
        "    ref_batch = torch.from_numpy(np.stack([np.load(f) for f in ref_filenames]))\n",
        "    return ref_batch\n",
        "\n",
        "  def fixed_test_audio(self, num_test_audio: int):\n",
        "    \"\"\"\n",
        "    Randomly chosen batch for testing generated results.\n",
        "    Args:\n",
        "        num_test_audio(int): number of test audio.\n",
        "            Must be same as batch size of training,\n",
        "            otherwise it cannot go through the forward step of generator.\n",
        "    \"\"\"\n",
        "    test_filenames = np.random.choice(self.filepaths, num_test_audio)\n",
        "    # stack the data for all test audios\n",
        "    test_audios = np.stack([np.load(f) for f in test_filenames])\n",
        "    test_clean_set = test_audios[:, 0].reshape((num_test_audio, 1, self.SAMPLE_LENGTH))\n",
        "    test_noisy_set = test_audios[:, 1].reshape((num_test_audio, 1, self.SAMPLE_LENGTH))\n",
        "    # file names of test samples\n",
        "    test_basenames = [os.path.basename(fpath) for fpath in test_filenames]\n",
        "    return test_basenames, test_clean_set, test_noisy_set\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # get item for specified index\n",
        "    pair = np.load(self.filepaths[idx])\n",
        "    return pair\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GI0wxeI1oXf",
        "colab_type": "text"
      },
      "source": [
        "# Virtual Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUK0Fi9HNYKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Virtual Batch Norm from https://github.com/dansuh17/segan-pytorch\n",
        "\n",
        "class VirtualBatchNorm1d(Module):\n",
        "    \"\"\"\n",
        "    Module for Virtual Batch Normalization.\n",
        "    Implementation borrowed and modified from Rafael_Valle's code + help of SimonW from this discussion thread:\n",
        "    https://discuss.pytorch.org/t/parameter-grad-of-conv-weight-is-none-after-virtual-batch-normalization/9036\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features: int, eps: float=1e-5):\n",
        "        super().__init__()\n",
        "        # batch statistics\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps  # epsilon\n",
        "        self.ref_mean = self.register_parameter('ref_mean', None)\n",
        "        self.ref_mean_sq = self.register_parameter('ref_mean_sq', None)\n",
        "\n",
        "        # define gamma and beta parameters\n",
        "        gamma = torch.normal(mean=torch.ones(1, num_features, 1), std=0.02)\n",
        "        self.gamma = Parameter(gamma.float().to(device))\n",
        "        self.beta = Parameter(torch.FloatTensor(1, num_features, 1).fill_(0)).to(device)\n",
        "\n",
        "    def get_stats(self, x):\n",
        "        \"\"\"\n",
        "        Calculates mean and mean square for given batch x.\n",
        "        Args:\n",
        "            x: tensor containing batch of activations\n",
        "        Returns:\n",
        "            mean: mean tensor over features\n",
        "            mean_sq: squared mean tensor over features\n",
        "        \"\"\"\n",
        "        mean = x.mean(2, keepdim=True).mean(0, keepdim=True)\n",
        "        mean_sq = (x ** 2).mean(2, keepdim=True).mean(0, keepdim=True)\n",
        "        return mean, mean_sq\n",
        "\n",
        "    def forward(self, x, ref_mean: None, ref_mean_sq: None):\n",
        "        \"\"\"\n",
        "        Forward pass of virtual batch normalization.\n",
        "        Virtual batch normalization require two forward passes\n",
        "        for reference batch and train batch, respectively.\n",
        "        The input parameter is_reference should indicate whether it is a forward pass\n",
        "        for reference batch or not.\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            is_reference(bool): True if forwarding for reference batch\n",
        "        Result:\n",
        "            x: normalized batch tensor\n",
        "        \"\"\"\n",
        "        mean, mean_sq = self.get_stats(x)\n",
        "        if ref_mean is None or ref_mean_sq is None:\n",
        "            # reference mode - works just like batch norm\n",
        "            mean = mean.clone().detach()\n",
        "            mean_sq = mean_sq.clone().detach()\n",
        "            out = self._normalize(x, mean, mean_sq)\n",
        "        else:\n",
        "            # calculate new mean and mean_sq\n",
        "            batch_size = x.size(0)\n",
        "            new_coeff = 1. / (batch_size + 1.)\n",
        "            old_coeff = 1. - new_coeff\n",
        "            mean = new_coeff * mean + old_coeff * ref_mean\n",
        "            mean_sq = new_coeff * mean_sq + old_coeff * ref_mean_sq\n",
        "            out = self._normalize(x, mean, mean_sq)\n",
        "        return out, mean, mean_sq\n",
        "\n",
        "    def _normalize(self, x, mean, mean_sq):\n",
        "        \"\"\"\n",
        "        Normalize tensor x given the statistics.\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            mean: mean over features. it has size [1:num_features:]\n",
        "            mean_sq: squared means over features.\n",
        "        Result:\n",
        "            x: normalized batch tensor\n",
        "        \"\"\"\n",
        "        assert mean_sq is not None\n",
        "        assert mean is not None\n",
        "        assert len(x.size()) == 3  # specific for 1d VBN\n",
        "        if mean.size(1) != self.num_features:\n",
        "            raise Exception(\n",
        "                    'Mean size not equal to number of featuers : given {}, expected {}'\n",
        "                    .format(mean.size(1), self.num_features))\n",
        "        if mean_sq.size(1) != self.num_features:\n",
        "            raise Exception(\n",
        "                    'Squared mean tensor size not equal to number of features : given {}, expected {}'\n",
        "                    .format(mean_sq.size(1), self.num_features))\n",
        "\n",
        "        std = torch.sqrt(self.eps + mean_sq - mean**2)\n",
        "        x = x - mean\n",
        "        x = x / std\n",
        "        x = x * self.gamma\n",
        "        x = x + self.beta\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return ('{name}(num_features={num_features}, eps={eps}'\n",
        "                .format(name=self.__class__.__name__, **self.__dict__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SnTQqyIxdSV",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlStmPBgxSA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator modified to take an 8KHz audio input\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"D\"\"\"\n",
        "    def __init__(self, dropout_drop=0.5):\n",
        "        super().__init__()\n",
        "        # Define convolution operations.\n",
        "        # (#input channel, #output channel, kernel_size, stride, padding)\n",
        "        # in : 16384 x 2\n",
        "        negative_slope = 0.03\n",
        "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=31, stride=2, padding=15)   # out : 8192 x 32, 4096 x 32\n",
        "        self.vbn1 = VirtualBatchNorm1d(32)\n",
        "        self.lrelu1 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 31, 2, 15)  # 4096 x 64, 2048, 64\n",
        "        self.vbn2 = VirtualBatchNorm1d(64)\n",
        "        self.lrelu2 = nn.LeakyReLU(negative_slope)\n",
        "        # self.conv3 = nn.Conv1d(64, 64, 31, 2, 15)  # 2048 x 64, # Removed to adjust for 8192 samples\n",
        "        # self.dropout1 = nn.Dropout(dropout_drop)\n",
        "        # self.vbn3 = VirtualBatchNorm1d(64)\n",
        "        # self.lrelu3 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv4 = nn.Conv1d(64, 128, 31, 2, 15)  # 1024 x 128, 1024 x 128\n",
        "        self.vbn4 = VirtualBatchNorm1d(128)\n",
        "        self.lrelu4 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv5 = nn.Conv1d(128, 128, 31, 2, 15)  # 512 x 128\n",
        "        self.vbn5 = VirtualBatchNorm1d(128)\n",
        "        self.lrelu5 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv6 = nn.Conv1d(128, 256, 31, 2, 15)  # 256 x 256\n",
        "        self.dropout2 = nn.Dropout(dropout_drop)\n",
        "        self.vbn6 = VirtualBatchNorm1d(256)\n",
        "        self.lrelu6 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv7 = nn.Conv1d(256, 256, 31, 2, 15)  # 128 x 256\n",
        "        self.vbn7 = VirtualBatchNorm1d(256)\n",
        "        self.lrelu7 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv8 = nn.Conv1d(256, 512, 31, 2, 15)  # 64 x 512\n",
        "        self.vbn8 = VirtualBatchNorm1d(512)\n",
        "        self.lrelu8 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv9 = nn.Conv1d(512, 512, 31, 2, 15)  # 32 x 512\n",
        "        self.dropout3 = nn.Dropout(dropout_drop)\n",
        "        self.vbn9 = VirtualBatchNorm1d(512)\n",
        "        self.lrelu9 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv10 = nn.Conv1d(512, 1024, 31, 2, 15)  # 16 x 1024\n",
        "        self.vbn10 = VirtualBatchNorm1d(1024)\n",
        "        self.lrelu10 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv11 = nn.Conv1d(1024, 2048, 31, 2, 15)  # 8 x 1024\n",
        "        self.vbn11 = VirtualBatchNorm1d(2048)\n",
        "        self.lrelu11 = nn.LeakyReLU(negative_slope)\n",
        "        # 1x1 size kernel for dimension and parameter reduction\n",
        "        self.conv_final = nn.Conv1d(2048, 1, kernel_size=1, stride=1)  # 8 x 1\n",
        "        self.lrelu_final = nn.LeakyReLU(negative_slope)\n",
        "        self.fully_connected = nn.Linear(in_features=8, out_features=1)  # 1\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights for convolution layers using Xavier initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, ref_x):\n",
        "        \"\"\"\n",
        "        Forward pass of discriminator.\n",
        "        Args:\n",
        "            x: batch\n",
        "            ref_x: reference batch for virtual batch norm\n",
        "        \"\"\"\n",
        "        # reference pass\n",
        "        ref_x = self.conv1(ref_x)\n",
        "        ref_x, mean1, meansq1 = self.vbn1(ref_x, None, None)\n",
        "        ref_x = self.lrelu1(ref_x)\n",
        "        ref_x = self.conv2(ref_x)\n",
        "        ref_x, mean2, meansq2 = self.vbn2(ref_x, None, None)\n",
        "        ref_x = self.lrelu2(ref_x)\n",
        "        # ref_x = self.conv3(ref_x)        # Removed to adjust for 8192 samples\n",
        "        # ref_x = self.dropout1(ref_x)\n",
        "        # ref_x, mean3, meansq3 = self.vbn3(ref_x, None, None)\n",
        "        # ref_x = self.lrelu3(ref_x)\n",
        "        ref_x = self.conv4(ref_x)\n",
        "        ref_x, mean4, meansq4 = self.vbn4(ref_x, None, None)\n",
        "        ref_x = self.lrelu4(ref_x)\n",
        "        ref_x = self.conv5(ref_x)\n",
        "        ref_x, mean5, meansq5 = self.vbn5(ref_x, None, None)\n",
        "        ref_x = self.lrelu5(ref_x)\n",
        "        ref_x = self.conv6(ref_x)\n",
        "        ref_x = self.dropout2(ref_x)\n",
        "        ref_x, mean6, meansq6 = self.vbn6(ref_x, None, None)\n",
        "        ref_x = self.lrelu6(ref_x)\n",
        "        ref_x = self.conv7(ref_x)\n",
        "        ref_x, mean7, meansq7 = self.vbn7(ref_x, None, None)\n",
        "        ref_x = self.lrelu7(ref_x)\n",
        "        ref_x = self.conv8(ref_x)\n",
        "        ref_x, mean8, meansq8 = self.vbn8(ref_x, None, None)\n",
        "        ref_x = self.lrelu8(ref_x)\n",
        "        ref_x = self.conv9(ref_x)\n",
        "        ref_x = self.dropout3(ref_x)\n",
        "        ref_x, mean9, meansq9 = self.vbn9(ref_x, None, None)\n",
        "        ref_x = self.lrelu9(ref_x)\n",
        "        ref_x = self.conv10(ref_x)\n",
        "        ref_x, mean10, meansq10 = self.vbn10(ref_x, None, None)\n",
        "        ref_x = self.lrelu10(ref_x)\n",
        "        ref_x = self.conv11(ref_x)\n",
        "        ref_x, mean11, meansq11 = self.vbn11(ref_x, None, None)\n",
        "        # further pass no longer needed\n",
        "\n",
        "        # train pass\n",
        "        x = self.conv1(x)\n",
        "        x, _, _ = self.vbn1(x, mean1, meansq1)\n",
        "        x = self.lrelu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x, _, _ = self.vbn2(x, mean2, meansq2)\n",
        "        x = self.lrelu2(x)\n",
        "        # x = self.conv3(x)        # Removed to adjust for 8192 samples\n",
        "        # x = self.dropout1(x)\n",
        "        # x, _, _ = self.vbn3(x, mean3, meansq3)\n",
        "        # x = self.lrelu3(x)\n",
        "        x = self.conv4(x)\n",
        "        x, _, _ = self.vbn4(x, mean4, meansq4)\n",
        "        x = self.lrelu4(x)\n",
        "        x = self.conv5(x)\n",
        "        x, _, _ = self.vbn5(x, mean5, meansq5)\n",
        "        x = self.lrelu5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.dropout2(x)\n",
        "        x, _, _ = self.vbn6(x, mean6, meansq6)\n",
        "        x = self.lrelu6(x)\n",
        "        x = self.conv7(x)\n",
        "        x, _, _ = self.vbn7(x, mean7, meansq7)\n",
        "        x = self.lrelu7(x)\n",
        "        x = self.conv8(x)\n",
        "        x, _, _ = self.vbn8(x, mean8, meansq8)\n",
        "        x = self.lrelu8(x)\n",
        "        x = self.conv9(x)\n",
        "        x = self.dropout3(x)\n",
        "        x, _, _ = self.vbn9(x, mean9, meansq9)\n",
        "        x = self.lrelu9(x)\n",
        "        x = self.conv10(x)\n",
        "        x, _, _ = self.vbn10(x, mean10, meansq10)\n",
        "        x = self.lrelu10(x)\n",
        "        x = self.conv11(x)\n",
        "        x, _, _ = self.vbn11(x, mean11, meansq11)\n",
        "        x = self.lrelu11(x)\n",
        "        x = self.conv_final(x)\n",
        "        x = self.lrelu_final(x)\n",
        "        # reduce down to a scalar value\n",
        "        x = torch.squeeze(x)\n",
        "        x = self.fully_connected(x)\n",
        "        # return self.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWqHrvnB3oDJ",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHZvgGJ0y_10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator modified to take an 8KHz audio input\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"G\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # size notations = [batch_size x feature_maps x width] (height omitted - 1D convolutions)\n",
        "        # encoder gets a noisy signal as input\n",
        "        self.enc1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=32, stride=2, padding=15)   # out : [B x 16 x 8192], [B x 16 x 4096]\n",
        "        self.enc1_nl = nn.PReLU()  # non-linear transformation after encoder layer 1\n",
        "        # self.enc2 = nn.Conv1d(16, 32, 32, 2, 15)  # [B x 32 x 4096] # Removed\n",
        "        # self.enc2_nl = nn.PReLU()\n",
        "        self.enc3 = nn.Conv1d(16, 32, 32, 2, 15)  # [B x 32 x 2048], [B x 32 x 2048]\n",
        "        self.enc3_nl = nn.PReLU()\n",
        "        self.enc4 = nn.Conv1d(32, 64, 32, 2, 15)  # [B x 64 x 1024] , cont as normal\n",
        "        self.enc4_nl = nn.PReLU()\n",
        "        self.enc5 = nn.Conv1d(64, 64, 32, 2, 15)  # [B x 64 x 512]\n",
        "        self.enc5_nl = nn.PReLU()\n",
        "        self.enc6 = nn.Conv1d(64, 128, 32, 2, 15)  # [B x 128 x 256]\n",
        "        self.enc6_nl = nn.PReLU()\n",
        "        self.enc7 = nn.Conv1d(128, 128, 32, 2, 15)  # [B x 128 x 128]\n",
        "        self.enc7_nl = nn.PReLU()\n",
        "        self.enc8 = nn.Conv1d(128, 256, 32, 2, 15)  # [B x 256 x 64]\n",
        "        self.enc8_nl = nn.PReLU()\n",
        "        self.enc9 = nn.Conv1d(256, 256, 32, 2, 15)  # [B x 256 x 32]\n",
        "        self.enc9_nl = nn.PReLU()\n",
        "        self.enc10 = nn.Conv1d(256, 512, 32, 2, 15)  # [B x 512 x 16]\n",
        "        self.enc10_nl = nn.PReLU()\n",
        "        self.enc11 = nn.Conv1d(512, 1024, 32, 2, 15)  # output : [B x 1024 x 8]\n",
        "        self.enc11_nl = nn.PReLU()\n",
        "\n",
        "        # decoder generates an enhanced signal\n",
        "        # each decoder output are concatenated with homolgous encoder output,\n",
        "        # so the feature map sizes are doubled\n",
        "        self.dec10 = nn.ConvTranspose1d(in_channels=2048, out_channels=512, kernel_size=32, stride=2, padding=15)\n",
        "        self.dec10_nl = nn.PReLU()  # out : [B x 512 x 16] -> (concat) [B x 1024 x 16]\n",
        "        self.dec9 = nn.ConvTranspose1d(1024, 256, 32, 2, 15)  # [B x 256 x 32]\n",
        "        self.dec9_nl = nn.PReLU()\n",
        "        self.dec8 = nn.ConvTranspose1d(512, 256, 32, 2, 15)  # [B x 256 x 64]\n",
        "        self.dec8_nl = nn.PReLU()\n",
        "        self.dec7 = nn.ConvTranspose1d(512, 128, 32, 2, 15)  # [B x 128 x 128]\n",
        "        self.dec7_nl = nn.PReLU()\n",
        "        self.dec6 = nn.ConvTranspose1d(256, 128, 32, 2, 15)  # [B x 128 x 256]\n",
        "        self.dec6_nl = nn.PReLU()\n",
        "        self.dec5 = nn.ConvTranspose1d(256, 64, 32, 2, 15)  # [B x 64 x 512]\n",
        "        self.dec5_nl = nn.PReLU()\n",
        "        self.dec4 = nn.ConvTranspose1d(128, 64, 32, 2, 15)  # [B x 64 x 1024]\n",
        "        self.dec4_nl = nn.PReLU()\n",
        "        self.dec3 = nn.ConvTranspose1d(128, 32, 32, 2, 15)  # [B x 32 x 2048]\n",
        "        self.dec3_nl = nn.PReLU()\n",
        "        # self.dec2 = nn.ConvTranspose1d(64, 32, 32, 2, 15)  # [B x 32 x 4096] # Same layer rmoved from encoder as well\n",
        "        # self.dec2_nl = nn.PReLU()\n",
        "        self.dec1 = nn.ConvTranspose1d(64, 16, 32, 2, 15)  # [B x 16 x 8192], [B x 16 x 4096]\n",
        "        self.dec1_nl = nn.PReLU()\n",
        "        self.dec_final = nn.ConvTranspose1d(32, 1, 32, 2, 15)  # [B x 1 x 16384], [B x 1 x 8192]\n",
        "        self.dec_tanh = nn.Tanh()\n",
        "\n",
        "        # initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights for convolution layers using Xavier initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        \"\"\"\n",
        "        Forward pass of generator.\n",
        "        Args:\n",
        "            x: input batch (signal)\n",
        "            z: latent vector\n",
        "        \"\"\"\n",
        "        ### encoding step\n",
        "        e1 = self.enc1(x)\n",
        "        # e2 = self.enc2(self.enc1_nl(e1))\n",
        "        e3 = self.enc3(self.enc1_nl(e1)) # e2 -> e1, enc2 -> enc1\n",
        "        e4 = self.enc4(self.enc3_nl(e3))\n",
        "        e5 = self.enc5(self.enc4_nl(e4))\n",
        "        e6 = self.enc6(self.enc5_nl(e5))\n",
        "        e7 = self.enc7(self.enc6_nl(e6))\n",
        "        e8 = self.enc8(self.enc7_nl(e7))\n",
        "        e9 = self.enc9(self.enc8_nl(e8))\n",
        "        e10 = self.enc10(self.enc9_nl(e9))\n",
        "        e11 = self.enc11(self.enc10_nl(e10))\n",
        "        # c = compressed feature, the 'thought vector'\n",
        "        c = self.enc11_nl(e11)\n",
        "\n",
        "        # concatenate the thought vector with latent variable\n",
        "        encoded = torch.cat((c, z), dim=1)\n",
        "\n",
        "        ### decoding step\n",
        "        d10 = self.dec10(encoded)\n",
        "        # dx_c : concatenated with skip-connected layer's output & passed nonlinear layer\n",
        "        d10_c = self.dec10_nl(torch.cat((d10, e10), dim=1))\n",
        "        d9 = self.dec9(d10_c)\n",
        "        d9_c = self.dec9_nl(torch.cat((d9, e9), dim=1))\n",
        "        d8 = self.dec8(d9_c)\n",
        "        d8_c = self.dec8_nl(torch.cat((d8, e8), dim=1))\n",
        "        d7 = self.dec7(d8_c)\n",
        "        d7_c = self.dec7_nl(torch.cat((d7, e7), dim=1))\n",
        "        d6 = self.dec6(d7_c)\n",
        "        d6_c = self.dec6_nl(torch.cat((d6, e6), dim=1))\n",
        "        d5 = self.dec5(d6_c)\n",
        "        d5_c = self.dec5_nl(torch.cat((d5, e5), dim=1))\n",
        "        d4 = self.dec4(d5_c)\n",
        "        d4_c = self.dec4_nl(torch.cat((d4, e4), dim=1))\n",
        "        d3 = self.dec3(d4_c)\n",
        "        d3_c = self.dec3_nl(torch.cat((d3, e3), dim=1))\n",
        "        # d2 = self.dec2(d3_c)\n",
        "        # d2_c = self.dec2_nl(torch.cat((d2, e2), dim=1))\n",
        "        d1 = self.dec1(d3_c) # d4_c -> d3_c\n",
        "        d1_c = self.dec1_nl(torch.cat((d1, e1), dim=1))\n",
        "        out = self.dec_tanh(self.dec_final(d1_c))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lge7wszx2fra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_pair_to_vars(sample_batch_pair):\n",
        "    \"\"\"\n",
        "    Splits the generated batch data and creates combination of pairs.\n",
        "    Input argument sample_batch_pair consists of a batch_size number of\n",
        "    [clean_signal, noisy_signal] pairs.\n",
        "    This function creates three pytorch Variables - a clean_signal, noisy_signal pair,\n",
        "    clean signal only, and noisy signal only.\n",
        "    It goes through preemphasis preprocessing before converted into variable.\n",
        "    Args:\n",
        "        sample_batch_pair(torch.Tensor): batch of [clean_signal, noisy_signal] pairs\n",
        "    Returns:\n",
        "        batch_pairs_var(Variable): batch of pairs containing clean signal and noisy signal\n",
        "        clean_batch_var(Variable): clean signal batch\n",
        "        noisy_batch_var(Varialbe): noisy signal batch\n",
        "    \"\"\"\n",
        "    # pre-emphasis\n",
        "    sample_batch_pair = pre_emphasis(sample_batch_pair.numpy())\n",
        "\n",
        "    batch_pairs_var = torch.from_numpy(sample_batch_pair).type(torch.FloatTensor).to(device)  # [40 x 2 x 16384]\n",
        "    clean_batch = np.stack([pair[0].reshape(1, -1) for pair in sample_batch_pair])\n",
        "    clean_batch_var = torch.from_numpy(clean_batch).type(torch.FloatTensor).to(device)\n",
        "    noisy_batch = np.stack([pair[1].reshape(1, -1) for pair in sample_batch_pair])\n",
        "    noisy_batch_var = torch.from_numpy(noisy_batch).type(torch.FloatTensor).to(device)\n",
        "    return batch_pairs_var, clean_batch_var, noisy_batch_var\n",
        "\n",
        "\n",
        "def sample_latent():\n",
        "    \"\"\"\n",
        "    Sample a latent vector - normal distribution\n",
        "    Returns:\n",
        "        z(torch.Tensor): random latent vector\n",
        "    \"\"\"\n",
        "    return torch.randn((batch_size, 1024, 8)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-9Abj864F48",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUP1JCpZ3zgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128 #(Trained at 256 batch_size at first, Colab failed and after epoch 7 had to use 128)\n",
        "d_learning_rate = 0.0001\n",
        "g_learning_rate = 0.0001\n",
        "g_lambda = 100  # regularizer for generator\n",
        "# use_devices = [0, 1, 2, 3]\n",
        "sample_rate = 8000\n",
        "num_epochs = 86 # Trained for only 13, before Colab revoked GPU access"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5yfOZyo7gN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "974ef415-bc60-4802-db10-81163bb59fbf"
      },
      "source": [
        "sample_generator = AudioSampleGenerator(f'{PATH}serialized/')\n",
        "random_data_loader = DataLoader(\n",
        "        dataset=sample_generator,\n",
        "        batch_size=batch_size,  # specified batch size here\n",
        "        shuffle=True,\n",
        "        drop_last=True,  # drop the last batch that cannot be divided by batch_size\n",
        "        pin_memory=False)\n",
        "print('DataLoader created')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataLoader created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMy6EmdN7on7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref_batch_pairs = sample_generator.reference_batch(batch_size)\n",
        "ref_batch_var, ref_clean_var, ref_noisy_var = split_pair_to_vars(ref_batch_pairs)\n",
        "\n",
        "# optimizers\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_learning_rate, betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ulMmZUu8NTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60d954f-f52f-4326-bcd9-d122633824b4"
      },
      "source": [
        "print('Starting Training...')\n",
        "total_steps = 1\n",
        "for epoch in range(13, num_epochs):\n",
        "    for i, sample_batch_pairs in enumerate(random_data_loader):\n",
        "        # using the sample batch pair, split into\n",
        "        # batch of combined pairs, clean signals, and noisy signals\n",
        "        batch_pairs_var, clean_batch_var, noisy_batch_var = split_pair_to_vars(sample_batch_pairs)\n",
        "\n",
        "        # latent vector - normal distribution\n",
        "        z = sample_latent()\n",
        "\n",
        "        ##### TRAIN D #####\n",
        "        # TRAIN D to recognize clean audio as clean\n",
        "        # training batch pass\n",
        "        outputs = discriminator(batch_pairs_var, ref_batch_var)  # out: [n_batch x 1]\n",
        "        clean_loss = torch.mean((outputs - 1.0) ** 2)  # L2 loss - we want them all to be 1\n",
        "\n",
        "        # TRAIN D to recognize generated audio as noisy\n",
        "        generated_outputs = generator(noisy_batch_var, z)\n",
        "        disc_in_pair = torch.cat((generated_outputs.detach(), noisy_batch_var), dim=1)\n",
        "        outputs = discriminator(disc_in_pair, ref_batch_var)\n",
        "        noisy_loss = torch.mean(outputs ** 2)  # L2 loss - we want them all to be 0\n",
        "        d_loss = 0.5 * (clean_loss + noisy_loss)\n",
        "\n",
        "        # back-propagate and update\n",
        "        discriminator.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()  # update parameters\n",
        "\n",
        "        ##### TRAIN G #####\n",
        "        # TRAIN G so that D recognizes G(z) as real\n",
        "        z = sample_latent()\n",
        "        generated_outputs = generator(noisy_batch_var, z)\n",
        "        gen_noise_pair = torch.cat((generated_outputs, noisy_batch_var), dim=1)\n",
        "        outputs = discriminator(gen_noise_pair, ref_batch_var)\n",
        "\n",
        "        g_loss_ = 0.5 * torch.mean((outputs - 1.0) ** 2)\n",
        "        # L1 loss between generated output and clean sample\n",
        "        l1_dist = torch.abs(torch.add(generated_outputs, torch.neg(clean_batch_var)))\n",
        "        g_cond_loss = g_lambda * torch.mean(l1_dist)  # conditional loss\n",
        "        g_loss = g_loss_ + g_cond_loss\n",
        "\n",
        "        # back-propagate and update\n",
        "        generator.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # print message and store logs per 10 steps\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(\n",
        "                'Epoch {}\\t'\n",
        "                'Step {}\\t'\n",
        "                'd_loss {:.5f}\\t'\n",
        "                'd_clean_loss {:.5f}\\t'\n",
        "                'd_noisy_loss {:.5f}\\t'\n",
        "                'g_loss {:.5f}\\t'\n",
        "                'g_loss_cond {:.5f}'\n",
        "                .format(epoch + 1, i + 1, d_loss.item(), clean_loss.item(),\n",
        "                        noisy_loss.item(), g_loss.item(), g_cond_loss.item()))\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "    # save various states\n",
        "    state_path = os.path.join(f'{PATH}/checkpoints', 'state-{}.pkl'.format(epoch + 1))\n",
        "    state = {\n",
        "        'discriminator': discriminator.state_dict(),\n",
        "        'generator': generator.state_dict(),\n",
        "        'g_optimizer': g_optimizer.state_dict(),\n",
        "        'd_optimizer': d_optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(state, state_path)\n",
        "\n",
        "    ### Can be loaded using, for example:\n",
        "    # states = torch.load(state_path)\n",
        "    # discriminator.load_state_dict(state['discriminator'])\n",
        "\n",
        "print('Finished Training!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training...\n",
            "Epoch 14\tStep 20\td_loss 0.28101\td_clean_loss 0.33827\td_noisy_loss 0.22375\tg_loss 1.05236\tg_loss_cond 0.88966\n",
            "Epoch 14\tStep 40\td_loss 0.26211\td_clean_loss 0.23245\td_noisy_loss 0.29176\tg_loss 1.03236\tg_loss_cond 0.91211\n",
            "Epoch 14\tStep 60\td_loss 0.25594\td_clean_loss 0.23028\td_noisy_loss 0.28161\tg_loss 1.02954\tg_loss_cond 0.92669\n",
            "Epoch 14\tStep 80\td_loss 0.24754\td_clean_loss 0.25868\td_noisy_loss 0.23641\tg_loss 1.07919\tg_loss_cond 0.92367\n",
            "Epoch 14\tStep 100\td_loss 0.25702\td_clean_loss 0.24930\td_noisy_loss 0.26473\tg_loss 0.97049\tg_loss_cond 0.84467\n",
            "Epoch 14\tStep 120\td_loss 0.24907\td_clean_loss 0.22422\td_noisy_loss 0.27393\tg_loss 1.01501\tg_loss_cond 0.90509\n",
            "Epoch 14\tStep 140\td_loss 0.24986\td_clean_loss 0.24379\td_noisy_loss 0.25593\tg_loss 1.03557\tg_loss_cond 0.91380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-zhky-1yFs",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTYy5YCuUR26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference function\n",
        "\n",
        "def test(filename):\n",
        "\n",
        "  # Initialise generator\n",
        "\n",
        "  generator = nn.DataParallel(Generator())\n",
        "  state = torch.load(f'{PATH}/checkpoints/state-13.pkl', map_location=device)\n",
        "  generator.load_state_dict(state['generator'])\n",
        "  generator.to(device)\n",
        "\n",
        "  # Read and slice audio input\n",
        "  noisy_slices = slice_signal(filename, 2**13, 1, 8000)\n",
        "  enhanced_speech = []\n",
        "  for noisy_slice in noisy_slices:\n",
        "    noisy_slice = noisy_slice.reshape(1, 1, 8192)\n",
        "    generator.eval()\n",
        "    z = nn.init.normal(torch.Tensor(1, 1024, 8))\n",
        "    noisy_slice = torch.from_numpy(pre_emphasis(noisy_slice)).type(torch.FloatTensor)\n",
        "    z.to(device)\n",
        "    noisy_slice.to(device)\n",
        "    generated_speech = generator(noisy_slice, z).data.cpu().numpy()\n",
        "    generated_speech = de_emphasis(generated_speech)\n",
        "    generated_speech = generated_speech.reshape(-1)\n",
        "    enhanced_speech.append(generated_speech)\n",
        "\n",
        "  enhanced_speech = np.array(enhanced_speech).reshape(1, -1)\n",
        "  name = filename.split('/')[-1]\n",
        "  filename = f'{PATH}/output/enhanced_{name}'\n",
        "  librosa.output.write_wav(filename, enhanced_speech.T, sr = 8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi7M6kHGRYqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a71fc6ee-f19d-463c-a136-c8f504d4171c"
      },
      "source": [
        "test(f'{PATH}/noisy_trainset_28spk_wav/p226_007.wav')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHWwLPjk-KtO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3e276c8e-f144-4f08-96bc-c4da1f31ee05"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.6.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTSlVRDEMEuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}