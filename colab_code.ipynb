{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEicEaVp1XSY",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cijGMCZgzhxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2036170f-97ab-4961-d8de-35ab3d9b4d99"
      },
      "source": [
        "import os\n",
        "PATH = r'/content/drive/My Drive/Saarthi.ai Assignment/'\n",
        "os.chdir(PATH)\n",
        "import librosa\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules import Module\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "\n",
        "# !pip install soundfile\n",
        "\n",
        "import soundfile\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjeMxsdQ1cVS",
        "colab_type": "text"
      },
      "source": [
        "# Audio Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bMKJM5o-aWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Audio inputted is sliced into segments of length 8192, with 50% overlap for train set, 0% overlap for inference. Padded 0s for packing the last 8192 samples.\n",
        "\n",
        "def slice_signal(filepath, window_size, stride, sample_rate = 8000):\n",
        "  wav, sr = librosa.load(filepath, sr = sample_rate)\n",
        "  if sr != 8000: print(f'Sampling rate error. Current rate is {sr}')\n",
        "  n_samples = wav.shape[0]  # contains simple amplitudes\n",
        "  hop = int(window_size * stride)\n",
        "  slices = []\n",
        "  wav = np.append(wav, [0] * (8192 - (len(wav) % 8192)))\n",
        "  for end_idx in range(window_size, len(wav) + 1, hop):\n",
        "      start_idx = end_idx - window_size\n",
        "      slice_sig = wav[start_idx:end_idx]\n",
        "      slices.append(slice_sig)\n",
        "  return slices"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWdpaFWlCA1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split audio each file's segments into single batches (.npy files) for easy loading during training.\n",
        "\n",
        "def serialize():\n",
        "\n",
        "  start_time = time.time()\n",
        "  window_size = 2 ** 13 # 8192 samples\n",
        "  sample_rate = 8000\n",
        "  stride = 0.5\n",
        "\n",
        "  clean_data_path = f'{PATH}/clean_trainset_28spk_wav/'\n",
        "  noisy_data_path = f'{PATH}/noisy_trainset_28spk_wav/'\n",
        "\n",
        "  for file in os.listdir(clean_data_path):\n",
        "\n",
        "    clean_filepath = f'{clean_data_path}{file}'\n",
        "    noisy_filepath = f'{noisy_data_path}{file}'\n",
        "\n",
        "    clean_sliced = slice_signal(clean_filepath, window_size, stride, sample_rate)\n",
        "    noisy_sliced = slice_signal(noisy_filepath, window_size, stride, sample_rate)\n",
        "\n",
        "    for idx, slice_tuple in enumerate(zip(clean_sliced, noisy_sliced)):\n",
        "      pair = np.array([slice_tuple[0], slice_tuple[1]])\n",
        "      np.save(os.path.join(f'{PATH}/serialized', '{}_{}'.format(file, idx)), arr=pair)\n",
        "\n",
        "  end_time = time.time()\n",
        "  print('Total elapsed time for preprocessing : {}'.format(end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCuavw_8Ey01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74cf606-e1d7-4ffe-9830-0981fe22a170"
      },
      "source": [
        "serialize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total elapsed time for preprocessing : 14714.036752700806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8guBf_7Mbt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-emphasis filter\n",
        "\n",
        "pre_emphasis = lambda batch: signal.lfilter([1, -0.95], [1], batch)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV3t6-_UM55Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "de_emphasis = lambda batch: signal.lfilter([1], [1, -0.95], batch)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRy8x5Q01hzZ",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHtsNWlcHCKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloader from https://github.com/dansuh17/segan-pytorch\n",
        "\n",
        "class AudioSampleGenerator(data.Dataset):\n",
        "  \"\"\"\n",
        "  Audio sample reader.\n",
        "  Used alongside with DataLoader class to generate batches.\n",
        "  see: http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\n",
        "  \"\"\"\n",
        "  SAMPLE_LENGTH = 8192\n",
        "\n",
        "  def __init__(self, data_folder_path: str):\n",
        "    if not os.path.exists(data_folder_path):\n",
        "      raise FileNotFoundError\n",
        "\n",
        "    # store full paths - not the actual files.\n",
        "    # all files cannot be loaded up to memory due to its large size.\n",
        "    # insted, we read from files upon fetching batches (see __getitem__() implementation)\n",
        "    self.filepaths = [os.path.join(data_folder_path, filename) for filename in os.listdir(data_folder_path)]\n",
        "    self.num_data = len(self.filepaths)\n",
        "\n",
        "  def reference_batch(self, batch_size: int):\n",
        "    \"\"\"\n",
        "    Randomly selects a reference batch from dataset.\n",
        "    Reference batch is used for calculating statistics for virtual batch normalization operation.\n",
        "    Args:\n",
        "        batch_size(int): batch size\n",
        "    Returns:\n",
        "        ref_batch: reference batch\n",
        "    \"\"\"\n",
        "    ref_filenames = np.random.choice(self.filepaths, batch_size)\n",
        "    ref_batch = torch.from_numpy(np.stack([np.load(f) for f in ref_filenames]))\n",
        "    return ref_batch\n",
        "\n",
        "  def fixed_test_audio(self, num_test_audio: int):\n",
        "    \"\"\"\n",
        "    Randomly chosen batch for testing generated results.\n",
        "    Args:\n",
        "        num_test_audio(int): number of test audio.\n",
        "            Must be same as batch size of training,\n",
        "            otherwise it cannot go through the forward step of generator.\n",
        "    \"\"\"\n",
        "    test_filenames = np.random.choice(self.filepaths, num_test_audio)\n",
        "    # stack the data for all test audios\n",
        "    test_audios = np.stack([np.load(f) for f in test_filenames])\n",
        "    test_clean_set = test_audios[:, 0].reshape((num_test_audio, 1, self.SAMPLE_LENGTH))\n",
        "    test_noisy_set = test_audios[:, 1].reshape((num_test_audio, 1, self.SAMPLE_LENGTH))\n",
        "    # file names of test samples\n",
        "    test_basenames = [os.path.basename(fpath) for fpath in test_filenames]\n",
        "    return test_basenames, test_clean_set, test_noisy_set\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # get item for specified index\n",
        "    pair = np.load(self.filepaths[idx])\n",
        "    return pair\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GI0wxeI1oXf",
        "colab_type": "text"
      },
      "source": [
        "# Virtual Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUK0Fi9HNYKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Virtual Batch Norm from https://github.com/dansuh17/segan-pytorch\n",
        "\n",
        "class VirtualBatchNorm1d(Module):\n",
        "    \"\"\"\n",
        "    Module for Virtual Batch Normalization.\n",
        "    Implementation borrowed and modified from Rafael_Valle's code + help of SimonW from this discussion thread:\n",
        "    https://discuss.pytorch.org/t/parameter-grad-of-conv-weight-is-none-after-virtual-batch-normalization/9036\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features: int, eps: float=1e-5):\n",
        "        super().__init__()\n",
        "        # batch statistics\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps  # epsilon\n",
        "        self.ref_mean = self.register_parameter('ref_mean', None)\n",
        "        self.ref_mean_sq = self.register_parameter('ref_mean_sq', None)\n",
        "\n",
        "        # define gamma and beta parameters\n",
        "        gamma = torch.normal(mean=torch.ones(1, num_features, 1), std=0.02)\n",
        "        self.gamma = Parameter(gamma.float().to(device))\n",
        "        self.beta = Parameter(torch.FloatTensor(1, num_features, 1).fill_(0)).to(device)\n",
        "\n",
        "    def get_stats(self, x):\n",
        "        \"\"\"\n",
        "        Calculates mean and mean square for given batch x.\n",
        "        Args:\n",
        "            x: tensor containing batch of activations\n",
        "        Returns:\n",
        "            mean: mean tensor over features\n",
        "            mean_sq: squared mean tensor over features\n",
        "        \"\"\"\n",
        "        mean = x.mean(2, keepdim=True).mean(0, keepdim=True)\n",
        "        mean_sq = (x ** 2).mean(2, keepdim=True).mean(0, keepdim=True)\n",
        "        return mean, mean_sq\n",
        "\n",
        "    def forward(self, x, ref_mean: None, ref_mean_sq: None):\n",
        "        \"\"\"\n",
        "        Forward pass of virtual batch normalization.\n",
        "        Virtual batch normalization require two forward passes\n",
        "        for reference batch and train batch, respectively.\n",
        "        The input parameter is_reference should indicate whether it is a forward pass\n",
        "        for reference batch or not.\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            is_reference(bool): True if forwarding for reference batch\n",
        "        Result:\n",
        "            x: normalized batch tensor\n",
        "        \"\"\"\n",
        "        mean, mean_sq = self.get_stats(x)\n",
        "        if ref_mean is None or ref_mean_sq is None:\n",
        "            # reference mode - works just like batch norm\n",
        "            mean = mean.clone().detach()\n",
        "            mean_sq = mean_sq.clone().detach()\n",
        "            out = self._normalize(x, mean, mean_sq)\n",
        "        else:\n",
        "            # calculate new mean and mean_sq\n",
        "            batch_size = x.size(0)\n",
        "            new_coeff = 1. / (batch_size + 1.)\n",
        "            old_coeff = 1. - new_coeff\n",
        "            mean = new_coeff * mean + old_coeff * ref_mean\n",
        "            mean_sq = new_coeff * mean_sq + old_coeff * ref_mean_sq\n",
        "            out = self._normalize(x, mean, mean_sq)\n",
        "        return out, mean, mean_sq\n",
        "\n",
        "    def _normalize(self, x, mean, mean_sq):\n",
        "        \"\"\"\n",
        "        Normalize tensor x given the statistics.\n",
        "        Args:\n",
        "            x: input tensor\n",
        "            mean: mean over features. it has size [1:num_features:]\n",
        "            mean_sq: squared means over features.\n",
        "        Result:\n",
        "            x: normalized batch tensor\n",
        "        \"\"\"\n",
        "        assert mean_sq is not None\n",
        "        assert mean is not None\n",
        "        assert len(x.size()) == 3  # specific for 1d VBN\n",
        "        if mean.size(1) != self.num_features:\n",
        "            raise Exception(\n",
        "                    'Mean size not equal to number of featuers : given {}, expected {}'\n",
        "                    .format(mean.size(1), self.num_features))\n",
        "        if mean_sq.size(1) != self.num_features:\n",
        "            raise Exception(\n",
        "                    'Squared mean tensor size not equal to number of features : given {}, expected {}'\n",
        "                    .format(mean_sq.size(1), self.num_features))\n",
        "\n",
        "        std = torch.sqrt(self.eps + mean_sq - mean**2)\n",
        "        x = x - mean\n",
        "        x = x / std\n",
        "        x = x * self.gamma\n",
        "        x = x + self.beta\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return ('{name}(num_features={num_features}, eps={eps}'\n",
        "                .format(name=self.__class__.__name__, **self.__dict__))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SnTQqyIxdSV",
        "colab_type": "text"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlStmPBgxSA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator modified to take an 8KHz audio input\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"D\"\"\"\n",
        "    def __init__(self, dropout_drop=0.5):\n",
        "        super().__init__()\n",
        "        # Define convolution operations.\n",
        "        # (#input channel, #output channel, kernel_size, stride, padding)\n",
        "        # in : 16384 x 2\n",
        "        negative_slope = 0.03\n",
        "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=31, stride=2, padding=15)   # out : 8192 x 32, 4096 x 32\n",
        "        self.vbn1 = VirtualBatchNorm1d(32)\n",
        "        self.lrelu1 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 31, 2, 15)  # 4096 x 64, 2048, 64\n",
        "        self.vbn2 = VirtualBatchNorm1d(64)\n",
        "        self.lrelu2 = nn.LeakyReLU(negative_slope)\n",
        "        # self.conv3 = nn.Conv1d(64, 64, 31, 2, 15)  # 2048 x 64, # Removed to adjust for 8192 samples\n",
        "        # self.dropout1 = nn.Dropout(dropout_drop)\n",
        "        # self.vbn3 = VirtualBatchNorm1d(64)\n",
        "        # self.lrelu3 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv4 = nn.Conv1d(64, 128, 31, 2, 15)  # 1024 x 128, 1024 x 128\n",
        "        self.vbn4 = VirtualBatchNorm1d(128)\n",
        "        self.lrelu4 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv5 = nn.Conv1d(128, 128, 31, 2, 15)  # 512 x 128\n",
        "        self.vbn5 = VirtualBatchNorm1d(128)\n",
        "        self.lrelu5 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv6 = nn.Conv1d(128, 256, 31, 2, 15)  # 256 x 256\n",
        "        self.dropout2 = nn.Dropout(dropout_drop)\n",
        "        self.vbn6 = VirtualBatchNorm1d(256)\n",
        "        self.lrelu6 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv7 = nn.Conv1d(256, 256, 31, 2, 15)  # 128 x 256\n",
        "        self.vbn7 = VirtualBatchNorm1d(256)\n",
        "        self.lrelu7 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv8 = nn.Conv1d(256, 512, 31, 2, 15)  # 64 x 512\n",
        "        self.vbn8 = VirtualBatchNorm1d(512)\n",
        "        self.lrelu8 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv9 = nn.Conv1d(512, 512, 31, 2, 15)  # 32 x 512\n",
        "        self.dropout3 = nn.Dropout(dropout_drop)\n",
        "        self.vbn9 = VirtualBatchNorm1d(512)\n",
        "        self.lrelu9 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv10 = nn.Conv1d(512, 1024, 31, 2, 15)  # 16 x 1024\n",
        "        self.vbn10 = VirtualBatchNorm1d(1024)\n",
        "        self.lrelu10 = nn.LeakyReLU(negative_slope)\n",
        "        self.conv11 = nn.Conv1d(1024, 2048, 31, 2, 15)  # 8 x 1024\n",
        "        self.vbn11 = VirtualBatchNorm1d(2048)\n",
        "        self.lrelu11 = nn.LeakyReLU(negative_slope)\n",
        "        # 1x1 size kernel for dimension and parameter reduction\n",
        "        self.conv_final = nn.Conv1d(2048, 1, kernel_size=1, stride=1)  # 8 x 1\n",
        "        self.lrelu_final = nn.LeakyReLU(negative_slope)\n",
        "        self.fully_connected = nn.Linear(in_features=8, out_features=1)  # 1\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights for convolution layers using Xavier initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, ref_x):\n",
        "        \"\"\"\n",
        "        Forward pass of discriminator.\n",
        "        Args:\n",
        "            x: batch\n",
        "            ref_x: reference batch for virtual batch norm\n",
        "        \"\"\"\n",
        "        # reference pass\n",
        "        ref_x = self.conv1(ref_x)\n",
        "        ref_x, mean1, meansq1 = self.vbn1(ref_x, None, None)\n",
        "        ref_x = self.lrelu1(ref_x)\n",
        "        ref_x = self.conv2(ref_x)\n",
        "        ref_x, mean2, meansq2 = self.vbn2(ref_x, None, None)\n",
        "        ref_x = self.lrelu2(ref_x)\n",
        "        # ref_x = self.conv3(ref_x)        # Removed to adjust for 8192 samples\n",
        "        # ref_x = self.dropout1(ref_x)\n",
        "        # ref_x, mean3, meansq3 = self.vbn3(ref_x, None, None)\n",
        "        # ref_x = self.lrelu3(ref_x)\n",
        "        ref_x = self.conv4(ref_x)\n",
        "        ref_x, mean4, meansq4 = self.vbn4(ref_x, None, None)\n",
        "        ref_x = self.lrelu4(ref_x)\n",
        "        ref_x = self.conv5(ref_x)\n",
        "        ref_x, mean5, meansq5 = self.vbn5(ref_x, None, None)\n",
        "        ref_x = self.lrelu5(ref_x)\n",
        "        ref_x = self.conv6(ref_x)\n",
        "        ref_x = self.dropout2(ref_x)\n",
        "        ref_x, mean6, meansq6 = self.vbn6(ref_x, None, None)\n",
        "        ref_x = self.lrelu6(ref_x)\n",
        "        ref_x = self.conv7(ref_x)\n",
        "        ref_x, mean7, meansq7 = self.vbn7(ref_x, None, None)\n",
        "        ref_x = self.lrelu7(ref_x)\n",
        "        ref_x = self.conv8(ref_x)\n",
        "        ref_x, mean8, meansq8 = self.vbn8(ref_x, None, None)\n",
        "        ref_x = self.lrelu8(ref_x)\n",
        "        ref_x = self.conv9(ref_x)\n",
        "        ref_x = self.dropout3(ref_x)\n",
        "        ref_x, mean9, meansq9 = self.vbn9(ref_x, None, None)\n",
        "        ref_x = self.lrelu9(ref_x)\n",
        "        ref_x = self.conv10(ref_x)\n",
        "        ref_x, mean10, meansq10 = self.vbn10(ref_x, None, None)\n",
        "        ref_x = self.lrelu10(ref_x)\n",
        "        ref_x = self.conv11(ref_x)\n",
        "        ref_x, mean11, meansq11 = self.vbn11(ref_x, None, None)\n",
        "        # further pass no longer needed\n",
        "\n",
        "        # train pass\n",
        "        x = self.conv1(x)\n",
        "        x, _, _ = self.vbn1(x, mean1, meansq1)\n",
        "        x = self.lrelu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x, _, _ = self.vbn2(x, mean2, meansq2)\n",
        "        x = self.lrelu2(x)\n",
        "        # x = self.conv3(x)        # Removed to adjust for 8192 samples\n",
        "        # x = self.dropout1(x)\n",
        "        # x, _, _ = self.vbn3(x, mean3, meansq3)\n",
        "        # x = self.lrelu3(x)\n",
        "        x = self.conv4(x)\n",
        "        x, _, _ = self.vbn4(x, mean4, meansq4)\n",
        "        x = self.lrelu4(x)\n",
        "        x = self.conv5(x)\n",
        "        x, _, _ = self.vbn5(x, mean5, meansq5)\n",
        "        x = self.lrelu5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.dropout2(x)\n",
        "        x, _, _ = self.vbn6(x, mean6, meansq6)\n",
        "        x = self.lrelu6(x)\n",
        "        x = self.conv7(x)\n",
        "        x, _, _ = self.vbn7(x, mean7, meansq7)\n",
        "        x = self.lrelu7(x)\n",
        "        x = self.conv8(x)\n",
        "        x, _, _ = self.vbn8(x, mean8, meansq8)\n",
        "        x = self.lrelu8(x)\n",
        "        x = self.conv9(x)\n",
        "        x = self.dropout3(x)\n",
        "        x, _, _ = self.vbn9(x, mean9, meansq9)\n",
        "        x = self.lrelu9(x)\n",
        "        x = self.conv10(x)\n",
        "        x, _, _ = self.vbn10(x, mean10, meansq10)\n",
        "        x = self.lrelu10(x)\n",
        "        x = self.conv11(x)\n",
        "        x, _, _ = self.vbn11(x, mean11, meansq11)\n",
        "        x = self.lrelu11(x)\n",
        "        x = self.conv_final(x)\n",
        "        x = self.lrelu_final(x)\n",
        "        # reduce down to a scalar value\n",
        "        x = torch.squeeze(x)\n",
        "        x = self.fully_connected(x)\n",
        "        # return self.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWqHrvnB3oDJ",
        "colab_type": "text"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHZvgGJ0y_10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator modified to take an 8KHz audio input\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"G\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # size notations = [batch_size x feature_maps x width] (height omitted - 1D convolutions)\n",
        "        # encoder gets a noisy signal as input\n",
        "        self.enc1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=32, stride=2, padding=15)   # out : [B x 16 x 8192], [B x 16 x 4096]\n",
        "        self.enc1_nl = nn.PReLU()  # non-linear transformation after encoder layer 1\n",
        "        # self.enc2 = nn.Conv1d(16, 32, 32, 2, 15)  # [B x 32 x 4096] # Removed\n",
        "        # self.enc2_nl = nn.PReLU()\n",
        "        self.enc3 = nn.Conv1d(16, 32, 32, 2, 15)  # [B x 32 x 2048], [B x 32 x 2048]\n",
        "        self.enc3_nl = nn.PReLU()\n",
        "        self.enc4 = nn.Conv1d(32, 64, 32, 2, 15)  # [B x 64 x 1024] , cont as normal\n",
        "        self.enc4_nl = nn.PReLU()\n",
        "        self.enc5 = nn.Conv1d(64, 64, 32, 2, 15)  # [B x 64 x 512]\n",
        "        self.enc5_nl = nn.PReLU()\n",
        "        self.enc6 = nn.Conv1d(64, 128, 32, 2, 15)  # [B x 128 x 256]\n",
        "        self.enc6_nl = nn.PReLU()\n",
        "        self.enc7 = nn.Conv1d(128, 128, 32, 2, 15)  # [B x 128 x 128]\n",
        "        self.enc7_nl = nn.PReLU()\n",
        "        self.enc8 = nn.Conv1d(128, 256, 32, 2, 15)  # [B x 256 x 64]\n",
        "        self.enc8_nl = nn.PReLU()\n",
        "        self.enc9 = nn.Conv1d(256, 256, 32, 2, 15)  # [B x 256 x 32]\n",
        "        self.enc9_nl = nn.PReLU()\n",
        "        self.enc10 = nn.Conv1d(256, 512, 32, 2, 15)  # [B x 512 x 16]\n",
        "        self.enc10_nl = nn.PReLU()\n",
        "        self.enc11 = nn.Conv1d(512, 1024, 32, 2, 15)  # output : [B x 1024 x 8]\n",
        "        self.enc11_nl = nn.PReLU()\n",
        "\n",
        "        # decoder generates an enhanced signal\n",
        "        # each decoder output are concatenated with homolgous encoder output,\n",
        "        # so the feature map sizes are doubled\n",
        "        self.dec10 = nn.ConvTranspose1d(in_channels=2048, out_channels=512, kernel_size=32, stride=2, padding=15)\n",
        "        self.dec10_nl = nn.PReLU()  # out : [B x 512 x 16] -> (concat) [B x 1024 x 16]\n",
        "        self.dec9 = nn.ConvTranspose1d(1024, 256, 32, 2, 15)  # [B x 256 x 32]\n",
        "        self.dec9_nl = nn.PReLU()\n",
        "        self.dec8 = nn.ConvTranspose1d(512, 256, 32, 2, 15)  # [B x 256 x 64]\n",
        "        self.dec8_nl = nn.PReLU()\n",
        "        self.dec7 = nn.ConvTranspose1d(512, 128, 32, 2, 15)  # [B x 128 x 128]\n",
        "        self.dec7_nl = nn.PReLU()\n",
        "        self.dec6 = nn.ConvTranspose1d(256, 128, 32, 2, 15)  # [B x 128 x 256]\n",
        "        self.dec6_nl = nn.PReLU()\n",
        "        self.dec5 = nn.ConvTranspose1d(256, 64, 32, 2, 15)  # [B x 64 x 512]\n",
        "        self.dec5_nl = nn.PReLU()\n",
        "        self.dec4 = nn.ConvTranspose1d(128, 64, 32, 2, 15)  # [B x 64 x 1024]\n",
        "        self.dec4_nl = nn.PReLU()\n",
        "        self.dec3 = nn.ConvTranspose1d(128, 32, 32, 2, 15)  # [B x 32 x 2048]\n",
        "        self.dec3_nl = nn.PReLU()\n",
        "        # self.dec2 = nn.ConvTranspose1d(64, 32, 32, 2, 15)  # [B x 32 x 4096] # Same layer rmoved from encoder as well\n",
        "        # self.dec2_nl = nn.PReLU()\n",
        "        self.dec1 = nn.ConvTranspose1d(64, 16, 32, 2, 15)  # [B x 16 x 8192], [B x 16 x 4096]\n",
        "        self.dec1_nl = nn.PReLU()\n",
        "        self.dec_final = nn.ConvTranspose1d(32, 1, 32, 2, 15)  # [B x 1 x 16384], [B x 1 x 8192]\n",
        "        self.dec_tanh = nn.Tanh()\n",
        "\n",
        "        # initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights for convolution layers using Xavier initialization.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d):\n",
        "                nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        \"\"\"\n",
        "        Forward pass of generator.\n",
        "        Args:\n",
        "            x: input batch (signal)\n",
        "            z: latent vector\n",
        "        \"\"\"\n",
        "        ### encoding step\n",
        "        e1 = self.enc1(x)\n",
        "        # e2 = self.enc2(self.enc1_nl(e1))\n",
        "        e3 = self.enc3(self.enc1_nl(e1)) # e2 -> e1, enc2 -> enc1\n",
        "        e4 = self.enc4(self.enc3_nl(e3))\n",
        "        e5 = self.enc5(self.enc4_nl(e4))\n",
        "        e6 = self.enc6(self.enc5_nl(e5))\n",
        "        e7 = self.enc7(self.enc6_nl(e6))\n",
        "        e8 = self.enc8(self.enc7_nl(e7))\n",
        "        e9 = self.enc9(self.enc8_nl(e8))\n",
        "        e10 = self.enc10(self.enc9_nl(e9))\n",
        "        e11 = self.enc11(self.enc10_nl(e10))\n",
        "        # c = compressed feature, the 'thought vector'\n",
        "        c = self.enc11_nl(e11)\n",
        "\n",
        "        # concatenate the thought vector with latent variable\n",
        "        encoded = torch.cat((c, z), dim=1)\n",
        "\n",
        "        ### decoding step\n",
        "        d10 = self.dec10(encoded)\n",
        "        # dx_c : concatenated with skip-connected layer's output & passed nonlinear layer\n",
        "        d10_c = self.dec10_nl(torch.cat((d10, e10), dim=1))\n",
        "        d9 = self.dec9(d10_c)\n",
        "        d9_c = self.dec9_nl(torch.cat((d9, e9), dim=1))\n",
        "        d8 = self.dec8(d9_c)\n",
        "        d8_c = self.dec8_nl(torch.cat((d8, e8), dim=1))\n",
        "        d7 = self.dec7(d8_c)\n",
        "        d7_c = self.dec7_nl(torch.cat((d7, e7), dim=1))\n",
        "        d6 = self.dec6(d7_c)\n",
        "        d6_c = self.dec6_nl(torch.cat((d6, e6), dim=1))\n",
        "        d5 = self.dec5(d6_c)\n",
        "        d5_c = self.dec5_nl(torch.cat((d5, e5), dim=1))\n",
        "        d4 = self.dec4(d5_c)\n",
        "        d4_c = self.dec4_nl(torch.cat((d4, e4), dim=1))\n",
        "        d3 = self.dec3(d4_c)\n",
        "        d3_c = self.dec3_nl(torch.cat((d3, e3), dim=1))\n",
        "        # d2 = self.dec2(d3_c)\n",
        "        # d2_c = self.dec2_nl(torch.cat((d2, e2), dim=1))\n",
        "        d1 = self.dec1(d3_c) # d4_c -> d3_c\n",
        "        d1_c = self.dec1_nl(torch.cat((d1, e1), dim=1))\n",
        "        out = self.dec_tanh(self.dec_final(d1_c))\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lge7wszx2fra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_pair_to_vars(sample_batch_pair):\n",
        "    \"\"\"\n",
        "    Splits the generated batch data and creates combination of pairs.\n",
        "    Input argument sample_batch_pair consists of a batch_size number of\n",
        "    [clean_signal, noisy_signal] pairs.\n",
        "    This function creates three pytorch Variables - a clean_signal, noisy_signal pair,\n",
        "    clean signal only, and noisy signal only.\n",
        "    It goes through preemphasis preprocessing before converted into variable.\n",
        "    Args:\n",
        "        sample_batch_pair(torch.Tensor): batch of [clean_signal, noisy_signal] pairs\n",
        "    Returns:\n",
        "        batch_pairs_var(Variable): batch of pairs containing clean signal and noisy signal\n",
        "        clean_batch_var(Variable): clean signal batch\n",
        "        noisy_batch_var(Varialbe): noisy signal batch\n",
        "    \"\"\"\n",
        "    # pre-emphasis\n",
        "    sample_batch_pair = pre_emphasis(sample_batch_pair.numpy())\n",
        "\n",
        "    batch_pairs_var = torch.from_numpy(sample_batch_pair).type(torch.FloatTensor).to(device)  # [40 x 2 x 16384]\n",
        "    clean_batch = np.stack([pair[0].reshape(1, -1) for pair in sample_batch_pair])\n",
        "    clean_batch_var = torch.from_numpy(clean_batch).type(torch.FloatTensor).to(device)\n",
        "    noisy_batch = np.stack([pair[1].reshape(1, -1) for pair in sample_batch_pair])\n",
        "    noisy_batch_var = torch.from_numpy(noisy_batch).type(torch.FloatTensor).to(device)\n",
        "    return batch_pairs_var, clean_batch_var, noisy_batch_var\n",
        "\n",
        "\n",
        "def sample_latent():\n",
        "    \"\"\"\n",
        "    Sample a latent vector - normal distribution\n",
        "    Returns:\n",
        "        z(torch.Tensor): random latent vector\n",
        "    \"\"\"\n",
        "    return torch.randn((batch_size, 1024, 8)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-9Abj864F48",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUP1JCpZ3zgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128 #(Trained at 256 batch_size at first, Colab failed and after epoch 7 had to use 128)\n",
        "d_learning_rate = 0.0001\n",
        "g_learning_rate = 0.0001\n",
        "g_lambda = 100  # regularizer for generator\n",
        "# use_devices = [0, 1, 2, 3]\n",
        "sample_rate = 8000\n",
        "num_epochs = 86 # Trained for only 13, before Colab revoked GPU access"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5yfOZyo7gN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "974ef415-bc60-4802-db10-81163bb59fbf"
      },
      "source": [
        "sample_generator = AudioSampleGenerator(f'{PATH}serialized/')\n",
        "random_data_loader = DataLoader(\n",
        "        dataset=sample_generator,\n",
        "        batch_size=batch_size,  # specified batch size here\n",
        "        shuffle=True,\n",
        "        drop_last=True,  # drop the last batch that cannot be divided by batch_size\n",
        "        pin_memory=False)\n",
        "print('DataLoader created')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataLoader created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMy6EmdN7on7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref_batch_pairs = sample_generator.reference_batch(batch_size)\n",
        "ref_batch_var, ref_clean_var, ref_noisy_var = split_pair_to_vars(ref_batch_pairs)\n",
        "\n",
        "# optimizers\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_learning_rate, betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ulMmZUu8NTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60d954f-f52f-4326-bcd9-d122633824b4"
      },
      "source": [
        "print('Starting Training...')\n",
        "total_steps = 1\n",
        "for epoch in range(13, num_epochs):\n",
        "    for i, sample_batch_pairs in enumerate(random_data_loader):\n",
        "        # using the sample batch pair, split into\n",
        "        # batch of combined pairs, clean signals, and noisy signals\n",
        "        batch_pairs_var, clean_batch_var, noisy_batch_var = split_pair_to_vars(sample_batch_pairs)\n",
        "\n",
        "        # latent vector - normal distribution\n",
        "        z = sample_latent()\n",
        "\n",
        "        ##### TRAIN D #####\n",
        "        # TRAIN D to recognize clean audio as clean\n",
        "        # training batch pass\n",
        "        outputs = discriminator(batch_pairs_var, ref_batch_var)  # out: [n_batch x 1]\n",
        "        clean_loss = torch.mean((outputs - 1.0) ** 2)  # L2 loss - we want them all to be 1\n",
        "\n",
        "        # TRAIN D to recognize generated audio as noisy\n",
        "        generated_outputs = generator(noisy_batch_var, z)\n",
        "        disc_in_pair = torch.cat((generated_outputs.detach(), noisy_batch_var), dim=1)\n",
        "        outputs = discriminator(disc_in_pair, ref_batch_var)\n",
        "        noisy_loss = torch.mean(outputs ** 2)  # L2 loss - we want them all to be 0\n",
        "        d_loss = 0.5 * (clean_loss + noisy_loss)\n",
        "\n",
        "        # back-propagate and update\n",
        "        discriminator.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()  # update parameters\n",
        "\n",
        "        ##### TRAIN G #####\n",
        "        # TRAIN G so that D recognizes G(z) as real\n",
        "        z = sample_latent()\n",
        "        generated_outputs = generator(noisy_batch_var, z)\n",
        "        gen_noise_pair = torch.cat((generated_outputs, noisy_batch_var), dim=1)\n",
        "        outputs = discriminator(gen_noise_pair, ref_batch_var)\n",
        "\n",
        "        g_loss_ = 0.5 * torch.mean((outputs - 1.0) ** 2)\n",
        "        # L1 loss between generated output and clean sample\n",
        "        l1_dist = torch.abs(torch.add(generated_outputs, torch.neg(clean_batch_var)))\n",
        "        g_cond_loss = g_lambda * torch.mean(l1_dist)  # conditional loss\n",
        "        g_loss = g_loss_ + g_cond_loss\n",
        "\n",
        "        # back-propagate and update\n",
        "        generator.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # print message and store logs per 10 steps\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(\n",
        "                'Epoch {}\\t'\n",
        "                'Step {}\\t'\n",
        "                'd_loss {:.5f}\\t'\n",
        "                'd_clean_loss {:.5f}\\t'\n",
        "                'd_noisy_loss {:.5f}\\t'\n",
        "                'g_loss {:.5f}\\t'\n",
        "                'g_loss_cond {:.5f}'\n",
        "                .format(epoch + 1, i + 1, d_loss.item(), clean_loss.item(),\n",
        "                        noisy_loss.item(), g_loss.item(), g_cond_loss.item()))\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "    # save various states\n",
        "    state_path = os.path.join(f'{PATH}/checkpoints', 'state-{}.pkl'.format(epoch + 1))\n",
        "    state = {\n",
        "        'discriminator': discriminator.state_dict(),\n",
        "        'generator': generator.state_dict(),\n",
        "        'g_optimizer': g_optimizer.state_dict(),\n",
        "        'd_optimizer': d_optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(state, state_path)\n",
        "\n",
        "    ### Can be loaded using, for example:\n",
        "    # states = torch.load(state_path)\n",
        "    # discriminator.load_state_dict(state['discriminator'])\n",
        "\n",
        "print('Finished Training!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training...\n",
            "Epoch 14\tStep 20\td_loss 0.28101\td_clean_loss 0.33827\td_noisy_loss 0.22375\tg_loss 1.05236\tg_loss_cond 0.88966\n",
            "Epoch 14\tStep 40\td_loss 0.26211\td_clean_loss 0.23245\td_noisy_loss 0.29176\tg_loss 1.03236\tg_loss_cond 0.91211\n",
            "Epoch 14\tStep 60\td_loss 0.25594\td_clean_loss 0.23028\td_noisy_loss 0.28161\tg_loss 1.02954\tg_loss_cond 0.92669\n",
            "Epoch 14\tStep 80\td_loss 0.24754\td_clean_loss 0.25868\td_noisy_loss 0.23641\tg_loss 1.07919\tg_loss_cond 0.92367\n",
            "Epoch 14\tStep 100\td_loss 0.25702\td_clean_loss 0.24930\td_noisy_loss 0.26473\tg_loss 0.97049\tg_loss_cond 0.84467\n",
            "Epoch 14\tStep 120\td_loss 0.24907\td_clean_loss 0.22422\td_noisy_loss 0.27393\tg_loss 1.01501\tg_loss_cond 0.90509\n",
            "Epoch 14\tStep 140\td_loss 0.24986\td_clean_loss 0.24379\td_noisy_loss 0.25593\tg_loss 1.03557\tg_loss_cond 0.91380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7-zhky-1yFs",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTYy5YCuUR26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference function\n",
        "\n",
        "def test(filename):\n",
        "\n",
        "  # Initialise generator\n",
        "\n",
        "  generator = nn.DataParallel(Generator())\n",
        "  state = torch.load(f'{PATH}/checkpoints/state-13.pkl', map_location=device)\n",
        "  generator.load_state_dict(state['generator'])\n",
        "  generator.to(device)\n",
        "\n",
        "  # Read and slice audio input\n",
        "  noisy_slices = slice_signal(filename, 2**13, 1, 8000)\n",
        "  enhanced_speech = []\n",
        "  for noisy_slice in noisy_slices:\n",
        "    noisy_slice = noisy_slice.reshape(1, 1, 8192)\n",
        "    generator.eval()\n",
        "    z = nn.init.normal(torch.Tensor(1, 1024, 8))\n",
        "    noisy_slice = torch.from_numpy(pre_emphasis(noisy_slice)).type(torch.FloatTensor)\n",
        "    z.to(device)\n",
        "    noisy_slice.to(device)\n",
        "    generated_speech = generator(noisy_slice, z).data.cpu().numpy()\n",
        "    generated_speech = de_emphasis(generated_speech)\n",
        "    generated_speech = generated_speech.reshape(-1)\n",
        "    enhanced_speech.append(generated_speech)\n",
        "\n",
        "  enhanced_speech = np.array(enhanced_speech).reshape(1, -1)\n",
        "  name = filename.split('/')[-1]\n",
        "  filename = f'{PATH}/output/enhanced_{name}'\n",
        "  librosa.output.write_wav(filename, enhanced_speech.T, sr = 8000)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi7M6kHGRYqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a71fc6ee-f19d-463c-a136-c8f504d4171c"
      },
      "source": [
        "test(f'{PATH}/noisy_trainset_28spk_wav/p226_007.wav')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPuLT9946HvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25670a88-e137-4418-f2aa-3c40117d2c4d"
      },
      "source": [
        "!pip list"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package                       Version        \n",
            "----------------------------- ---------------\n",
            "absl-py                       0.9.0          \n",
            "alabaster                     0.7.12         \n",
            "albumentations                0.1.12         \n",
            "altair                        4.1.0          \n",
            "argon2-cffi                   20.1.0         \n",
            "asgiref                       3.2.10         \n",
            "astor                         0.8.1          \n",
            "astropy                       4.0.1.post1    \n",
            "astunparse                    1.6.3          \n",
            "atari-py                      0.2.6          \n",
            "atomicwrites                  1.4.0          \n",
            "attrs                         19.3.0         \n",
            "audioread                     2.1.8          \n",
            "autograd                      1.3            \n",
            "Babel                         2.8.0          \n",
            "backcall                      0.2.0          \n",
            "beautifulsoup4                4.6.3          \n",
            "bleach                        3.1.5          \n",
            "blis                          0.4.1          \n",
            "bokeh                         2.1.1          \n",
            "boto                          2.49.0         \n",
            "boto3                         1.14.37        \n",
            "botocore                      1.17.37        \n",
            "Bottleneck                    1.3.2          \n",
            "branca                        0.4.1          \n",
            "bs4                           0.0.1          \n",
            "CacheControl                  0.12.6         \n",
            "cachetools                    4.1.1          \n",
            "catalogue                     1.0.0          \n",
            "certifi                       2020.6.20      \n",
            "cffi                          1.14.1         \n",
            "chainer                       7.4.0          \n",
            "chardet                       3.0.4          \n",
            "click                         7.1.2          \n",
            "cloudpickle                   1.3.0          \n",
            "cmake                         3.12.0         \n",
            "cmdstanpy                     0.4.0          \n",
            "colorlover                    0.3.0          \n",
            "community                     1.0.0b1        \n",
            "contextlib2                   0.5.5          \n",
            "convertdate                   2.2.1          \n",
            "coverage                      3.7.1          \n",
            "coveralls                     0.5            \n",
            "crcmod                        1.7            \n",
            "cufflinks                     0.17.3         \n",
            "cvxopt                        1.2.5          \n",
            "cvxpy                         1.0.31         \n",
            "cycler                        0.10.0         \n",
            "cymem                         2.0.3          \n",
            "Cython                        0.29.21        \n",
            "daft                          0.0.4          \n",
            "dask                          2.12.0         \n",
            "dataclasses                   0.7            \n",
            "datascience                   0.10.6         \n",
            "decorator                     4.4.2          \n",
            "defusedxml                    0.6.0          \n",
            "descartes                     1.1.0          \n",
            "dill                          0.3.2          \n",
            "distributed                   1.25.3         \n",
            "Django                        3.1            \n",
            "dlib                          19.18.0        \n",
            "dm-tree                       0.1.5          \n",
            "docopt                        0.6.2          \n",
            "docutils                      0.15.2         \n",
            "dopamine-rl                   1.0.5          \n",
            "earthengine-api               0.1.229        \n",
            "easydict                      1.9            \n",
            "ecos                          2.0.7.post1    \n",
            "editdistance                  0.5.3          \n",
            "en-core-web-sm                2.2.5          \n",
            "entrypoints                   0.3            \n",
            "ephem                         3.7.7.1        \n",
            "et-xmlfile                    1.0.1          \n",
            "fa2                           0.3.5          \n",
            "fancyimpute                   0.4.3          \n",
            "fastai                        1.0.61         \n",
            "fastdtw                       0.3.4          \n",
            "fastprogress                  0.2.5          \n",
            "fastrlock                     0.5            \n",
            "fbprophet                     0.6            \n",
            "feather-format                0.4.1          \n",
            "featuretools                  0.4.1          \n",
            "filelock                      3.0.12         \n",
            "firebase-admin                4.1.0          \n",
            "fix-yahoo-finance             0.0.22         \n",
            "Flask                         1.1.2          \n",
            "folium                        0.8.3          \n",
            "fsspec                        0.8.0          \n",
            "future                        0.16.0         \n",
            "gast                          0.3.3          \n",
            "GDAL                          2.2.2          \n",
            "gdown                         3.6.4          \n",
            "gensim                        3.6.0          \n",
            "geographiclib                 1.50           \n",
            "geopy                         1.17.0         \n",
            "gin-config                    0.3.0          \n",
            "glob2                         0.7            \n",
            "google                        2.0.3          \n",
            "google-api-core               1.16.0         \n",
            "google-api-python-client      1.7.12         \n",
            "google-auth                   1.17.2         \n",
            "google-auth-httplib2          0.0.4          \n",
            "google-auth-oauthlib          0.4.1          \n",
            "google-cloud-bigquery         1.21.0         \n",
            "google-cloud-core             1.0.3          \n",
            "google-cloud-datastore        1.8.0          \n",
            "google-cloud-firestore        1.7.0          \n",
            "google-cloud-language         1.2.0          \n",
            "google-cloud-storage          1.18.1         \n",
            "google-cloud-translate        1.5.0          \n",
            "google-colab                  1.0.0          \n",
            "google-pasta                  0.2.0          \n",
            "google-resumable-media        0.4.1          \n",
            "googleapis-common-protos      1.52.0         \n",
            "googledrivedownloader         0.4            \n",
            "graphviz                      0.10.1         \n",
            "grpcio                        1.31.0         \n",
            "gspread                       3.0.1          \n",
            "gspread-dataframe             3.0.7          \n",
            "gym                           0.17.2         \n",
            "h5py                          2.10.0         \n",
            "HeapDict                      1.0.1          \n",
            "holidays                      0.9.12         \n",
            "holoviews                     1.13.3         \n",
            "html5lib                      1.0.1          \n",
            "httpimport                    0.5.18         \n",
            "httplib2                      0.17.4         \n",
            "httplib2shim                  0.0.3          \n",
            "humanize                      0.5.1          \n",
            "hyperopt                      0.1.2          \n",
            "ideep4py                      2.0.0.post3    \n",
            "idna                          2.10           \n",
            "image                         1.5.32         \n",
            "imageio                       2.4.1          \n",
            "imagesize                     1.2.0          \n",
            "imbalanced-learn              0.4.3          \n",
            "imblearn                      0.0            \n",
            "imgaug                        0.2.9          \n",
            "importlib-metadata            1.7.0          \n",
            "imutils                       0.5.3          \n",
            "inflect                       2.1.0          \n",
            "iniconfig                     1.0.1          \n",
            "intel-openmp                  2020.0.133     \n",
            "intervaltree                  2.1.0          \n",
            "ipykernel                     4.10.1         \n",
            "ipython                       5.5.0          \n",
            "ipython-genutils              0.2.0          \n",
            "ipython-sql                   0.3.9          \n",
            "ipywidgets                    7.5.1          \n",
            "itsdangerous                  1.1.0          \n",
            "jax                           0.1.75         \n",
            "jaxlib                        0.1.52         \n",
            "jdcal                         1.4.1          \n",
            "jedi                          0.17.2         \n",
            "jieba                         0.42.1         \n",
            "Jinja2                        2.11.2         \n",
            "jmespath                      0.10.0         \n",
            "joblib                        0.16.0         \n",
            "jpeg4py                       0.1.4          \n",
            "jsonschema                    2.6.0          \n",
            "jupyter                       1.0.0          \n",
            "jupyter-client                5.3.5          \n",
            "jupyter-console               5.2.0          \n",
            "jupyter-core                  4.6.3          \n",
            "kaggle                        1.5.6          \n",
            "kapre                         0.1.3.1        \n",
            "Keras                         2.4.3          \n",
            "Keras-Preprocessing           1.1.2          \n",
            "keras-vis                     0.4.1          \n",
            "kiwisolver                    1.2.0          \n",
            "knnimpute                     0.1.0          \n",
            "librosa                       0.6.3          \n",
            "lightgbm                      2.2.3          \n",
            "llvmlite                      0.31.0         \n",
            "lmdb                          0.98           \n",
            "lucid                         0.3.8          \n",
            "LunarCalendar                 0.0.9          \n",
            "lxml                          4.2.6          \n",
            "Markdown                      3.2.2          \n",
            "MarkupSafe                    1.1.1          \n",
            "matplotlib                    3.2.2          \n",
            "matplotlib-venn               0.11.5         \n",
            "missingno                     0.4.2          \n",
            "mistune                       0.8.4          \n",
            "mizani                        0.6.0          \n",
            "mkl                           2019.0         \n",
            "mlxtend                       0.14.0         \n",
            "more-itertools                8.4.0          \n",
            "moviepy                       0.2.3.5        \n",
            "mpmath                        1.1.0          \n",
            "msgpack                       1.0.0          \n",
            "multiprocess                  0.70.10        \n",
            "multitasking                  0.0.9          \n",
            "murmurhash                    1.0.2          \n",
            "music21                       5.5.0          \n",
            "natsort                       5.5.0          \n",
            "nbconvert                     5.6.1          \n",
            "nbformat                      5.0.7          \n",
            "networkx                      2.4            \n",
            "nibabel                       3.0.2          \n",
            "nltk                          3.2.5          \n",
            "notebook                      5.3.1          \n",
            "np-utils                      0.5.12.1       \n",
            "numba                         0.48.0         \n",
            "numexpr                       2.7.1          \n",
            "numpy                         1.18.5         \n",
            "nvidia-ml-py3                 7.352.0        \n",
            "oauth2client                  4.1.3          \n",
            "oauthlib                      3.1.0          \n",
            "okgrade                       0.4.3          \n",
            "opencv-contrib-python         4.1.2.30       \n",
            "opencv-python                 4.1.2.30       \n",
            "openpyxl                      2.5.9          \n",
            "opt-einsum                    3.3.0          \n",
            "osqp                          0.6.1          \n",
            "packaging                     20.4           \n",
            "palettable                    3.3.0          \n",
            "pandas                        1.0.5          \n",
            "pandas-datareader             0.8.1          \n",
            "pandas-gbq                    0.11.0         \n",
            "pandas-profiling              1.4.1          \n",
            "pandocfilters                 1.4.2          \n",
            "panel                         0.9.7          \n",
            "param                         1.9.3          \n",
            "parso                         0.7.1          \n",
            "pathlib                       1.0.1          \n",
            "patsy                         0.5.1          \n",
            "pexpect                       4.8.0          \n",
            "pickleshare                   0.7.5          \n",
            "Pillow                        7.0.0          \n",
            "pip                           19.3.1         \n",
            "pip-tools                     4.5.1          \n",
            "plac                          1.1.3          \n",
            "plotly                        4.4.1          \n",
            "plotnine                      0.6.0          \n",
            "pluggy                        0.7.1          \n",
            "portpicker                    1.3.1          \n",
            "prefetch-generator            1.0.1          \n",
            "preshed                       3.0.2          \n",
            "prettytable                   0.7.2          \n",
            "progressbar2                  3.38.0         \n",
            "prometheus-client             0.8.0          \n",
            "promise                       2.3            \n",
            "prompt-toolkit                1.0.18         \n",
            "protobuf                      3.12.4         \n",
            "psutil                        5.4.8          \n",
            "psycopg2                      2.7.6.1        \n",
            "ptyprocess                    0.6.0          \n",
            "py                            1.9.0          \n",
            "pyarrow                       0.14.1         \n",
            "pyasn1                        0.4.8          \n",
            "pyasn1-modules                0.2.8          \n",
            "pycocotools                   2.0.1          \n",
            "pycparser                     2.20           \n",
            "pyct                          0.4.6          \n",
            "pydata-google-auth            1.1.0          \n",
            "pydot                         1.3.0          \n",
            "pydot-ng                      2.0.0          \n",
            "pydotplus                     2.0.2          \n",
            "PyDrive                       1.3.1          \n",
            "pyemd                         0.5.1          \n",
            "pyglet                        1.5.0          \n",
            "Pygments                      2.1.3          \n",
            "pygobject                     3.26.1         \n",
            "pymc3                         3.7            \n",
            "PyMeeus                       0.3.7          \n",
            "pymongo                       3.11.0         \n",
            "pymystem3                     0.2.0          \n",
            "PyOpenGL                      3.1.5          \n",
            "pyparsing                     2.4.7          \n",
            "pyrsistent                    0.16.0         \n",
            "pysndfile                     1.3.8          \n",
            "PySocks                       1.7.1          \n",
            "pystan                        2.19.1.1       \n",
            "pytest                        3.6.4          \n",
            "python-apt                    1.6.5+ubuntu0.3\n",
            "python-chess                  0.23.11        \n",
            "python-dateutil               2.8.1          \n",
            "python-louvain                0.14           \n",
            "python-slugify                4.0.1          \n",
            "python-utils                  2.4.0          \n",
            "pytz                          2018.9         \n",
            "pyviz-comms                   0.7.6          \n",
            "PyWavelets                    1.1.1          \n",
            "PyYAML                        3.13           \n",
            "pyzmq                         19.0.2         \n",
            "qtconsole                     4.7.5          \n",
            "QtPy                          1.9.0          \n",
            "regex                         2019.12.20     \n",
            "requests                      2.23.0         \n",
            "requests-oauthlib             1.3.0          \n",
            "resampy                       0.2.2          \n",
            "retrying                      1.3.3          \n",
            "rpy2                          3.2.7          \n",
            "rsa                           4.6            \n",
            "s3fs                          0.4.2          \n",
            "s3transfer                    0.3.3          \n",
            "scikit-image                  0.16.2         \n",
            "scikit-learn                  0.22.2.post1   \n",
            "scipy                         1.4.1          \n",
            "screen-resolution-extra       0.0.0          \n",
            "scs                           2.1.2          \n",
            "seaborn                       0.10.1         \n",
            "Send2Trash                    1.5.0          \n",
            "setuptools                    49.2.0         \n",
            "setuptools-git                1.2            \n",
            "Shapely                       1.7.0          \n",
            "simplegeneric                 0.8.1          \n",
            "six                           1.15.0         \n",
            "sklearn                       0.0            \n",
            "sklearn-pandas                1.8.0          \n",
            "smart-open                    2.1.0          \n",
            "snowballstemmer               2.0.0          \n",
            "sortedcontainers              2.2.2          \n",
            "SoundFile                     0.10.3.post1   \n",
            "spacy                         2.2.4          \n",
            "Sphinx                        1.8.5          \n",
            "sphinxcontrib-serializinghtml 1.1.4          \n",
            "sphinxcontrib-websupport      1.2.3          \n",
            "SQLAlchemy                    1.3.18         \n",
            "sqlparse                      0.3.1          \n",
            "srsly                         1.0.2          \n",
            "statsmodels                   0.10.2         \n",
            "sympy                         1.1.1          \n",
            "tables                        3.4.4          \n",
            "tabulate                      0.8.7          \n",
            "tblib                         1.7.0          \n",
            "tensorboard                   2.3.0          \n",
            "tensorboard-plugin-wit        1.7.0          \n",
            "tensorboardcolab              0.0.22         \n",
            "tensorflow                    2.3.0          \n",
            "tensorflow-addons             0.8.3          \n",
            "tensorflow-datasets           2.1.0          \n",
            "tensorflow-estimator          2.3.0          \n",
            "tensorflow-gcs-config         2.3.0          \n",
            "tensorflow-hub                0.8.0          \n",
            "tensorflow-metadata           0.22.2         \n",
            "tensorflow-privacy            0.2.2          \n",
            "tensorflow-probability        0.11.0         \n",
            "termcolor                     1.1.0          \n",
            "terminado                     0.8.3          \n",
            "testpath                      0.4.4          \n",
            "text-unidecode                1.3            \n",
            "textblob                      0.15.3         \n",
            "textgenrnn                    1.4.1          \n",
            "Theano                        1.0.5          \n",
            "thinc                         7.4.0          \n",
            "tifffile                      2020.7.24      \n",
            "toml                          0.10.1         \n",
            "toolz                         0.10.0         \n",
            "torch                         1.6.0+cu101    \n",
            "torchsummary                  1.5.1          \n",
            "torchtext                     0.3.1          \n",
            "torchvision                   0.7.0+cu101    \n",
            "tornado                       5.1.1          \n",
            "tqdm                          4.41.1         \n",
            "traitlets                     4.3.3          \n",
            "tweepy                        3.6.0          \n",
            "typeguard                     2.7.1          \n",
            "typing-extensions             3.7.4.2        \n",
            "tzlocal                       1.5.1          \n",
            "umap-learn                    0.4.6          \n",
            "uritemplate                   3.0.1          \n",
            "urllib3                       1.24.3         \n",
            "vega-datasets                 0.8.0          \n",
            "wasabi                        0.7.1          \n",
            "wcwidth                       0.2.5          \n",
            "webencodings                  0.5.1          \n",
            "Werkzeug                      1.0.1          \n",
            "wheel                         0.34.2         \n",
            "widgetsnbextension            3.5.1          \n",
            "wordcloud                     1.5.0          \n",
            "wrapt                         1.12.1         \n",
            "xarray                        0.15.1         \n",
            "xgboost                       0.90           \n",
            "xkit                          0.0.0          \n",
            "xlrd                          1.1.0          \n",
            "xlwt                          1.3.0          \n",
            "yellowbrick                   0.9.1          \n",
            "zict                          2.0.0          \n",
            "zipp                          3.1.0          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMwT9-Jv6JQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch 1.6.0+cu101 \n",
        "numpy 1.18.5\n",
        "scipy 1.4.1\n",
        "SoundFile 0.10.3.post1 \n",
        "librosa 0.63"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}